{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Houses Sale Price \n",
    "\n",
    "**Raneem AlMindeel**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "- [Abstract](#Abstract)\n",
    "- [1. Introduction](#1.-Introduction)\n",
    "- [2. The Data](#2.-The-Data)\n",
    "    - [2.1 Import the Data](#2.1-Import-the-Data)\n",
    "    - [2.2 Data Exploration](#2.2-Data-Exploration)\n",
    "    - [2.3 Data Preparation](#2.3-Data-Preparation)\n",
    "    - [2.4 Correlation](#2.4-Correlation)\n",
    "- [3. Project Description](#3.-Project-Description)\n",
    "    - [3.1 Linear Regression](#3.1-Linear-Regression)\n",
    "    - [3.2 Analysis](#3.2-Analysis)\n",
    "    - [3.3 Results](#3.3-Results)\n",
    "    - [3.4 Verify Your Model Against Test Data](#3.4-Verify-Your-Model-Against-Test-Data)\n",
    "- [Conclusion](#Conclusion)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "\n",
    "In this project , the author has attempted to explore and analyze the hosing sales data using the Linear regression model. There were several steps taken to undergo such process including exploring the dataset,cleaning and refining the data, gathering the model requirements, as well as buiding and verifying the results of developed model. The results has shown that the developed model is a good predictor (with accuracy score of 0.85) of houses selling prices and can be used to gather additional statistics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "## Abstract \n",
    "The following project report has attempted to find an accurate method to predict Houses selling prices.The steps needed were importing , preparing and cleaning data, develping a prediction model , running and testing the quaity of the model. The results has shown that the developed regression model with the selected factors is a good predictor of the selling pirce of houses which can be further utilized to drive deeper insights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "## 1. Introduction\n",
    "This project explored the possiblilty of predicting housses selling prices using one or more factors. The steps carried have achieved the task in hand by making such prediction. \n",
    "The steps taken to perform this project, are as follows: \n",
    "1. Data import \n",
    "2. Data exploration\n",
    "3. Data refinements\n",
    "4. Selecting desired model (Linear Regression Model)\n",
    "5. Idetifying requirements (dependent and independent varibles)\n",
    "6. Gathering requirements \n",
    "7. Builing the regression model \n",
    "8. Testing and verifying the model \n",
    "9. Plotitng the required model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "## 2. The Data\n",
    "\n",
    "The following is a summary of the work carried: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.1 Import the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "#read the data from the required dataset \n",
    "data = pd.read_csv(‘houseSmallData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.2 Data Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore given data set and its underlying shape (I.e. number of rows and columns).\n",
    "data.shape\n",
    "#create a subset of the dataset rows\n",
    "train = data.iloc[0:100,:] \n",
    "train.shape \n",
    "#to find the numeric columns \n",
    "numeric = train.select_dtypes(include=[np.number])\n",
    "numeric.shape \n",
    "#to find the correlations of such numeric columns\n",
    "corr = numeric.corr()\n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10].index\n",
    "cols\n",
    "#investigate the variable needs to be predicted, that is the selling price (Y-dependent variable)\n",
    "train[‘SalePrice’].head() \n",
    "#to verify that all 100 values are present and gather more info about the Y-variable.   \n",
    "salePrice = train['SalePrice']#train is a datagrame and salePrice is a dataseries. \n",
    "salePrice.describe()\n",
    "plt.hist(salePrice)#clearly the sale price is not normally distributed , but fairly skewed try scaling it by computing the log of sale price, the hist shown is now can be seen as normally distributed.  \n",
    "plt.hist(np.log(salePrice))\n",
    "#Note: if we could predict the log of a sale price, we could certainly predict the sale price.\n",
    "salePrice.skew()#1.18 is a quite significant \n",
    "np.log(salePrice).skew()#around 0.1,the skwenees is reduced significantly(more evenly distributed than skewed)\n",
    "#enhanced sale price variable \n",
    "target = np.log(salePrice)\n",
    "#selecting an independent variable and exploring its relationship with sp by plotting a scatter graph \n",
    "plt.scatter(train['GrLivArea'], y=target) \n",
    "#Need to fix the plotting, the y axis should be from 10.5 to 13, x-axis from 400 to 3100. \n",
    "plt.ylim([10.45, 13.1])\n",
    "plt.xlim([400, 3100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.3 Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying null columns and Cleaning the data (dropping columns with all nulls)\n",
    "nulls=pd.DataFrame(train.isnull().sum().sort_values(ascending=False))[0:16]\n",
    "data= train.select_dtypes(include=[np.number]).interpolate().dropna(axis=1)\n",
    "sum(data.isnull().sum()!=0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 2.4 Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "#to select the best independent variables , that is the ones highly correlated with the dependent variable (selling price)\n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10]#column names with their corresponding correlations.  \n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10].index #only column names \n",
    "cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to top](#Index)\n",
    "\n",
    "## 3. Project Description\n",
    "\n",
    "The following are the additional improvements: \n",
    "Cleaning the data (if necessary). The selected independent variables were explored and none of the include Null values ; therefore , no adjustments were made.\n",
    "Exploring additional independent variables (Upon calculating the correlation of variables in the dataset, several new columns were selected since they have high correlation with the dependent variable (I.e. houses selling price). \n",
    "Data visualization via histograms and scatter plots was used to explore and visualize needed results. \n",
    "\n",
    "In order to determine the desired X-independent variables, first identify the numeric columns.Second, find the correlations of such numeric columns with respect to the dependent variable.Last, select the numeric columns which are highly correlated with the Y-variable.  The selected variables are , as follows: \n",
    "‘OverallQual’,’GrLivArea’,’GarageArea',‘GarageCars’,’YearBuilt’,’ TotalBsmtSF’’FullBath’,’MasVnrArea'\n",
    "The selection of indepndent varibles , has used the following correlation results: \n",
    "According to the results of calculating the correlation between the houses selling prices and the remaining variables, the following is observed: \n",
    "The following code is used: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=corr['SalePrice'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Overall quality shows the highest correlation with a correlation coefficient of 0.86.\n",
    "GrLivArea shows a high correlation with a correlation coefficient of 0.74.\n",
    "GarageArea shows a high correlation with a correlation coefficient of 0.69.\n",
    "GarageCars shows a high correlation with a correlation coefficient of 0.663.\n",
    "YearBuilt shows a high correlation with a correlation coefficient of 0.659.\n",
    "TotalBsmtSF shows a high correlation with a correlation coefficient of 0.62.\n",
    "GarageYrBlt shows a high correlation with a correlation coefficient of 0.60.\n",
    "FullBath shows a high correlation with a correlation coefficient of 0.58.\n",
    "MasVnrArea shows a high correlation with a correlation coefficient of 0.57.\n",
    "TotRmsAbvGrd shows a high correlation with a correlation coefficient of 0.554.\n",
    "1stFlrSF shows a high correlation with a correlation coefficient of 0.551.\n",
    "YearRemodAdd shows a high correlation with a correlation coefficient of 0.548.\n",
    "LotArea shows a high correlation with a correlation coefficient of 0.50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.1 Linear Regression\n",
    "\n",
    "The alogrithem used in this project is the linear regression model , which is a  \n",
    "a method to predict a result (y) using another variable (x) through a linear function as , follows :  y=c0+c1x1+c2x2+...+ckxk   (Zhao, 2013)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.2 Analysis \n",
    "\n",
    "Detailed steps carries, are as follows: \n",
    "Step 1: Explore the data set in question. That is by reading the data , and identifying its underlying shape (number of rows and columns). \n",
    "\n",
    "Step 2 : Investigate the attributes provided in the data set. That is, by identifying the type of columns , selecting the numeric columns, creating a subset of the data to implement the model on (e.g. 20 to 30 % out of the total number of records in the data set) \n",
    "\n",
    "Step 3: identify the requirements needed to build the regression model. \n",
    "That is by performing the following: \n",
    "\n",
    "a. Determine and investigate the dependent variable needed for prediction, that is the houses selling price (Y-dependent variable). Plot a histogram for the selected  variable, for further clarification.  \n",
    "b.Determine the desired X-independent variables. To do that : first identify the numeric columns.Second, find the correlations of such numeric columns with respect to the dependent variable.Last, select the numeric columns which are highly correlated with the Y-variable.  The selected variables are , as follows: \n",
    "‘OverallQual’,’GrLivArea’,’GarageArea',‘GarageCars’,’YearBuilt’,’ TotalBsmtSF’’FullBath’,’MasVnrArea'\n",
    "\n",
    " c. Upon identifying the dependent and independent variables, import necessary libraries and build the Linear regression model\n",
    "\n",
    "Step 4 : Building the regression model. This entails creating a linear model variable that invokes the Linear Regression library , fit the data and make the necessary prediction.\n",
    "\n",
    "Step 5: Model testing and verification. \n",
    "This step entails verifying and testing how good the model is (or the made predictions per se), by testing it with other set of data (other than the selected train data).Such testing will make use of the score() function which returns the mean accuracy score of our model. This will tell us how well the selling price (Y) is predicted by the chosen independent variables. \n",
    "\n",
    "#Note : the required factor is better between 0.8 and 0.9.\n",
    "\n",
    "Step 6: Plotting the predictions. In this step, use the scatter graph type to plot the resulted Y predictions (i.e. houses selling price). \n",
    "\n",
    "The following are the additional improvements: \n",
    "Cleaning the data (if necessary). The selected independent variables were explored and none of the include Null values ; therefore , no adjustments were made.\n",
    "Exploring additional independent variables (Upon calculating the correlation of variables in the dataset, several new columns were selected since they have high correlation with the dependent variable (I.e. houses selling price). \n",
    "Data visualization via histograms and scatter plots was used to explore and visualize needed results. \n",
    "Full code , as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "#read the data from the required dataset \n",
    "\n",
    "data = pd.read_csv(‘houseSmallData.csv')\n",
    "#explore given data set and its underlying shape (I.e. number of rows and columns).\n",
    "data.shape\n",
    "#create a subset of the dataset rows\n",
    "train = data.iloc[0:100,:] \n",
    "train.shape \n",
    "#to find the numeric columns \n",
    "numeric = train.select_dtypes(include=[np.number])\n",
    "numeric.shape \n",
    "#to find the correlations of such numeric columns\n",
    "corr = numeric.corr()\n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10].index\n",
    "cols\n",
    "#investigate the variable needs to be predicted, that is the selling price (Y-dependent variable)\n",
    "train[‘SalePrice’].head() \n",
    "#to verify that all 100 values are present and gather more info about the Y-variable.   \n",
    "salePrice = train['SalePrice']#train is a datagrame and salePrice is a dataseries. \n",
    "salePrice.describe()\n",
    "plt.hist(salePrice)#clearly the sale price is not normally distributed , but fairly skewed try scaling it by computing the log of sale price, the hist shown is now can be seen as normally distributed.  \n",
    "plt.hist(np.log(salePrice))\n",
    "#Note: if we could predict the log of a sale price, we could certainly predict the sale price.\n",
    "salePrice.skew()#1.18 is a quite significant \n",
    "np.log(salePrice).skew()#around 0.1,the skwenees is reduced significantly(more evenly distributed than skewed)\n",
    "#enhanced sale price variable \n",
    "target = np.log(salePrice)\n",
    "#selecting an independent variable and exploring its relationship with sp by plotting a scatter graph \n",
    "plt.scatter(train['GrLivArea'], y=target) \n",
    "#Need to fix the plotting, the y axis should be from 10.5 to 13, x-axis from 400 to 3100. \n",
    "plt.ylim([10.45, 13.1])\n",
    "plt.xlim([400, 3100])\n",
    "#Identifying null columns and Cleaning the data (dropping columns with all nulls)\n",
    "nulls=pd.DataFrame(train.isnull().sum().sort_values(ascending=False))[0:16]\n",
    "data= train.select_dtypes(include=[np.number]).interpolate().dropna(axis=1)\n",
    "sum(data.isnull().sum()!=0)\n",
    "data.shape\n",
    "corr = data.corr()\n",
    "#to select the best independent variables , that is the ones highly correlated with the dependent variable (selling price)\n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10]#column names with their corresponding correlations.  \n",
    "cols=corr['SalePrice'].sort_values(ascending=False)[0:10].index #only column names \n",
    "cols\n",
    "\n",
    "#setting up the model \n",
    "#to identify the dependent and independent variables (X cols and Y=SalePrice).\n",
    "X = data[cols]\n",
    "Y = X['SalePrice']\n",
    "X = X.drop(['SalePrice'],axis=1)\n",
    "#to remove the column with null values \n",
    "X = X.drop([‘GarageYrBlt'],axis=1)\n",
    "\n",
    "#importing necessary library and building the Linear regression model \n",
    "from sklearn import linear_model \n",
    "#create the linear model\n",
    "lr = linear_model.LinearRegression()\n",
    "#fit the data\n",
    "model = lr.fit(X,Y) \n",
    "predictions = model.predict(X)\n",
    "#to calculate score, the coefficient of determination(R2), which is normally called on regressors to assess their performance. \n",
    "print(f\"R^2 is:{model.score(X,Y)}”)\n",
    "#Note: the model score obtained is around 0.85 , which is considered to be a good result and a good indication of the strength of prediction of the developed regression model.\n",
    "#plot the predictions \n",
    "plt.hist(Y - predictions)#now it it looks like a normal distribution \n",
    "#scatter plot the predictions \n",
    "plt.scatter(predictions,Y, color='r')\n",
    "plt.ylim([30000, 455000])\n",
    "plt.xlim([30000, 360000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.3 Results\n",
    "\n",
    "1. After plotting the histogram of the salePrice column or series , it appears that it is not normally distributed ; however, it is fairly skewed. That is , it is not evenly distributed about any point here.  \n",
    "      Solution: scaling it , by taking the log of the sale price. (Because, if we\n",
    "      could predict the log of a sale price, we could certainly predict the sale price).\n",
    "\n",
    "2. The conversion to the log of sale price is better since according to the following: salePrice.skew() has a skewness of 1.18 , which is quite significant. However, the log of the sale price is better since (np.log(salePrice).skew()) shows a skewness level of around 0.1, which is reduced significantly.That is , it is more evenly distributed than skewed. \n",
    "      target = np.log(salePrice)\n",
    "\n",
    "3. Making sure whether the chosen variables have null values or not. \n",
    " Code:  nulls as a dataframe \n",
    " nulls=pd.DataFrame(train.isnull().sum().sort_values(ascending=False))[0:16] \n",
    "    nulls\n",
    "Or \n",
    "nulls as a dataseries \n",
    "nulls=train.isnull().sum().sort_values(ascending=False)[0:16] \n",
    "nulls\n",
    "\n",
    "If any of the independent variables are chosen, and it happens they have null values. Then, we better repair them or remove them. \n",
    "\n",
    "4. Calculating the model sore , the result of including 8 most correlated columns is 85% , which is considered good. This means that 85% of the price edge is explained by the chosen independent variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "### 3.4 Verify Your Model Against Test Data\n",
    "Testing the developed model using another set of data to see how good our model is (i.e. score)\n",
    "The idea is to verify how good the model is , by testing it with other set of data (other than the selected train data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use another data set to test your model (test dataset : ‘jtest.csv’)\n",
    "test = pd.read_csv('jtest.csv')\n",
    "test.shape\n",
    "test.head()\n",
    "X=test[cols]\n",
    "Y = X['SalePrice']\n",
    "X = X.drop(['SalePrice'],axis=1)\n",
    "#to remove the column with null values \n",
    "X = X.drop([‘GarageYrBlt'],axis=1)\n",
    "#test using the score() function which returns the mean accuracy score of our model. This will tell us how well the selling price (Y) is predicted by the selected independent variables. \n",
    "predictions = model.predict(X)\n",
    "print(f\"R^2 for testing data is:{model.score(X,Y)}”)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Index)\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Upon calculating the model accuracy sore , the result of including 8 most correlated columns is 85% , which is considered good. This means that 85% of the price edge is explained by the chosen independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Back to top](#Index)\n",
    "## References\n",
    "\n",
    "- Doe, John. “Data Engineering.” Data Engineer Resource. Cengage, 2021. www.dataengineerresource.com .\n",
    "\n",
    "- Zhao, Y. (2013). Regression . In R and data mining. essay, Academic Press. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
